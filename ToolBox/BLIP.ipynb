{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3635f50-9d4b-48ec-8d7d-f8c1e09358a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665f5db83a8f47afb06dad94a9c2d02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define QnA function\n",
    "def ask_image_question(image_path, question):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(image, question=question, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a7808-6d6f-424b-a8f4-06d5f38c711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "openhermes_path = r\"C:\\GGUF\\TheBloke\\OpenHermes-2.5-Mistral-7B-GGUF\\openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\n",
    "\n",
    "OpenHermes = Llama(\n",
    "    model_path=openhermes_path,\n",
    "    n_gpu_layers=20,\n",
    "    n_ctx=2048,\n",
    "    n_batch=256,\n",
    "    n_threads=6,\n",
    "    use_mlock=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c83e6df-1d28-4ea9-806b-17acad3c993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip2_to_openhermes(image_path, question):\n",
    "    print(f\"‚ùì QnA: {question}\")\n",
    "    \n",
    "    visual_answer = ask_image_question(image_path, question)\n",
    "    print(\"üì∏ BLIP-2 Answer:\", visual_answer)\n",
    "\n",
    "    hermes_prompt = (\n",
    "        f\"The image was analyzed and the answer to the question \"\n",
    "        f\"'{question}' is: '{visual_answer}'. Can you provide a deeper interpretation?\"\n",
    "    )\n",
    "    full_prompt = f\"<|user|>\\n{hermes_prompt}\\n<|assistant|>\\n\"\n",
    "\n",
    "    response = OpenHermes(full_prompt, max_tokens=300, stop=[\"<|user|>\"])\n",
    "    hermes_text = response[\"choices\"][0][\"text\"]\n",
    "\n",
    "    print(\"üß† OpenHermes Response:\", hermes_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98d9a40-d1a5-4378-b610-c2533065a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì QnA: Describe this image\n",
      "üì∏ BLIP-2 Answer: a man is sitting in a chair with a laptop computer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 43 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7002.10 ms\n",
      "llama_perf_context_print: prompt eval time =    2736.97 ms /     9 tokens (  304.11 ms per token,     3.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   91928.63 ms /   110 runs   (  835.71 ms per token,     1.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   96191.11 ms /   119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† OpenHermes Response: The image portrays a modern scene of a man using technology to perform various tasks or connect with others. The man's posture and the arrangement of the laptop on his lap suggest that he may be working or communicating with someone remotely. The surroundings are not visible in the image, but the man's attire and the laptop indicate that this could be happening in an office or a home setting. Overall, the image depicts a common scenario of a person using technology as a tool for productivity and interaction in the digital age.\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"C:\\Users\\DaysPC\\Pictures\\Screenshots\\PFP.jpg\"\n",
    "question = \"Describe this image\"\n",
    "blip2_to_openhermes(image_path, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c37e05-0aca-4065-b7d1-deba834177c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310-cuda]",
   "language": "python",
   "name": "conda-env-py310-cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
