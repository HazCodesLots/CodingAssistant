{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998eb351-9b1c-4336-b3d0-bd8a9fbb8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "openhermes_path = r\"C:\\Users\\DaysPC\\Documents\\Datasets\\Transformers\\openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\n",
    "deepseek_path = r\"C:\\Users\\DaysPC\\Documents\\Datasets\\Transformers\\deepseek-coder-6.7b-instruct-q4_k_m.gguf\"\n",
    "\n",
    "HISTORY_DIR = r\"D:\\\\Jupytor\\\\Wraps\\\\Tessa\\\\History\"\n",
    "os.makedirs(HISTORY_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37befdb-8239-4388-9872-e3cdb68f9e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OpenHermes = Llama(\n",
    "    model_path=openhermes_path,\n",
    "    n_gpu_layers=20,\n",
    "    n_ctx=2048,\n",
    "    n_batch=256,\n",
    "    n_threads=6,\n",
    "    use_mlock=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86832264-6df9-4cf1-9c31-e153d5ed123e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DeepSeekCode = Llama(\n",
    "    model_path=deepseek_path,\n",
    "    n_gpu_layers=20,\n",
    "    n_ctx=2048,\n",
    "    n_batch=256,\n",
    "    n_threads=6,\n",
    "    use_mlock=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd668f1-2dad-40e7-aecc-2a79a773d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(session_history, current_input, for_model=\"OpenHermes\"):\n",
    "    prompt = \"\"\n",
    "\n",
    "    if for_model == \"OpenHermes\":\n",
    "        \n",
    "        for user, assistant, model_name in session_history:\n",
    "            if model_name == \"OpenHermes\":\n",
    "                prompt += f\"<|user|>\\n{user}\\n<|assistant|>\\n{assistant}\\n\"\n",
    "        prompt += f\"<|user|>\\n{current_input}\\n<|assistant|>\\n\"\n",
    "\n",
    "    elif for_model == \"DeepSeekCode\":\n",
    "        \n",
    "        for user, assistant, model_name in session_history:\n",
    "            if model_name == \"DeepSeekCode\":\n",
    "                prompt += f\"User: {user}\\nAssistant: {assistant}\\n\"\n",
    "        prompt += f\"User: {current_input}\\nAssistant:\"\n",
    "\n",
    "    else:\n",
    "        \n",
    "        for user, assistant, model_name in session_history:\n",
    "            prompt += f\"{user}\\n{assistant}\\n\"\n",
    "        prompt += f\"{current_input}\\n\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c016951-b78a-4ffa-9f55-ed7b395164bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input, session_history, coding_mode):\n",
    "    model = DeepSeekCode if coding_mode else OpenHermes\n",
    "    model_name = \"DeepSeekCode\" if coding_mode else \"OpenHermes\"\n",
    "\n",
    "    \n",
    "    for_model = \"OpenHermes\" if not coding_mode else \"DeepSeekCode\"\n",
    "    prompt = format_prompt(session_history, user_input, for_model=for_model)\n",
    "\n",
    "    \n",
    "    prompt_token_est = len(prompt.split())  \n",
    "    max_total_tokens = 2048\n",
    "    generation_tokens = max_total_tokens - prompt_token_est\n",
    "\n",
    "    \n",
    "    if not coding_mode:\n",
    "        \n",
    "        generation_tokens = min(generation_tokens, 512)\n",
    "    else:\n",
    "        \n",
    "        generation_tokens = max(256, min(generation_tokens, 1536))\n",
    "\n",
    "    \n",
    "    output = model(prompt, max_tokens=generation_tokens, stop=None if coding_mode else [\"<|user|>\", \"<|assistant|>\"])\n",
    "    response = output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    session_history.append((user_input, response, model_name))\n",
    "    return session_history, gr.update(value=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71146f0b-ad12-4ad7-9552-58443aeb8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chat(history):\n",
    "    if not history:\n",
    "        return\n",
    "    filename = os.path.join(HISTORY_DIR, f\"chat_{len(os.listdir(HISTORY_DIR)) + 1}.json\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "def load_chat(file):\n",
    "    if file is None:\n",
    "        return []\n",
    "    with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f8f02d-3be0-412c-8d6b-ce3775d9b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DaysPC\\AppData\\Local\\Temp\\ipykernel_8620\\1764846062.py:100: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(elem_id=\"chatbot\", height=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "css = \"\"\"\n",
    "/* Clear background for chat */\n",
    "#chatbot .message.user {\n",
    "    background-color: transparent !important;\n",
    "    color: #d1d5db;\n",
    "    font-weight: 500;\n",
    "}\n",
    "#chatbot .message.bot {\n",
    "    background-color: transparent !important;\n",
    "    color: #ffffff;\n",
    "}\n",
    "\n",
    "/* Input styling */\n",
    "#input-row {\n",
    "    display: flex;\n",
    "    margin-top: 10px;\n",
    "}\n",
    "#textbox {\n",
    "    flex-grow: 1;\n",
    "    background-color: #1f1f1f;\n",
    "    color: #eee;\n",
    "}\n",
    "\n",
    "/* Button styling */\n",
    "#save-btn, #open-btn {\n",
    "    height: 60px !important;\n",
    "    font-size: 16px;\n",
    "    padding: 10px 16px;\n",
    "}\n",
    "\n",
    "/* Send Button Blue */\n",
    "#send-btn {\n",
    "    background-color: #007BFF !important;\n",
    "    color: white !important;\n",
    "    border: none !important;\n",
    "}\n",
    "#send-btn:hover {\n",
    "    background-color: #0056b3 !important;\n",
    "}\n",
    "\n",
    "/* Remove chatbot box style */\n",
    "#chatbot {\n",
    "    background: transparent !important;\n",
    "    box-shadow: none !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def chat(user_input, session_history, coding_mode):\n",
    "    model = DeepSeekCode if coding_mode else OpenHermes\n",
    "    model_name = \"DeepSeekCode\" if coding_mode else \"OpenHermes\"\n",
    "    for_model = \"OpenHermes\" if not coding_mode else \"DeepSeekCode\"\n",
    "    prompt = format_prompt(session_history, user_input, for_model=for_model)\n",
    "\n",
    "    prompt_token_est = len(prompt.split())\n",
    "    max_total_tokens = 2048\n",
    "    generation_tokens = (\n",
    "        min(max_total_tokens - prompt_token_est, 512)\n",
    "        if not coding_mode else max(256, min(max_total_tokens - prompt_token_est, 1536))\n",
    "    )\n",
    "\n",
    "    output = model(prompt, max_tokens=generation_tokens, stop=None if coding_mode else [\"<|user|>\", \"<|assistant|>\"])\n",
    "    response = output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    \n",
    "    session_history.append((user_input, response, model_name))\n",
    "\n",
    "    \n",
    "    chatbot_display = [[u, a] for u, a, _ in session_history]\n",
    "\n",
    "    return chatbot_display, gr.update(value=\"\")\n",
    "\n",
    "\n",
    "def load_chat(file):\n",
    "    if file is None:\n",
    "        return [], []\n",
    "    with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  \n",
    "    chatbot_display = [[u, a] for u, a, _ in data]  # strip model_name\n",
    "    return data, chatbot_display  \n",
    "\n",
    "\n",
    "def save_chat(history):\n",
    "    if not history:\n",
    "        return\n",
    "    filename = os.path.join(HISTORY_DIR, f\"chat_{len(os.listdir(HISTORY_DIR)) + 1}.json\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"<h2 style='color:#007BFF;'>ðŸ’¬ OpenHermes Chat (GTX 1650m)</h2>\")\n",
    "        with gr.Row():\n",
    "            open_btn = gr.Button(\"ðŸ“‚ Open Chat\", elem_id=\"open-btn\", scale=1)\n",
    "            save_btn = gr.Button(\"ðŸ’¾ Save Chat\", elem_id=\"save-btn\", scale=1)\n",
    "\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\", height=400)\n",
    "    state = gr.State([])\n",
    "    coding_mode = gr.State(False)\n",
    "\n",
    "    with gr.Row(elem_id=\"input-row\"):\n",
    "        txt = gr.Textbox(placeholder=\"Type here...\", show_label=False, container=False, elem_id=\"textbox\")\n",
    "        send = gr.Button(\"Send\", elem_id=\"send-btn\")\n",
    "\n",
    "    \n",
    "    coding_btn = gr.Button(\"ðŸ’» Coding Mode: OFF\", elem_id=\"send-btn\")  \n",
    "\n",
    "    def toggle_coding(current):\n",
    "        new_mode = not current\n",
    "        label = \"ðŸ’» Coding Mode: ON\" if new_mode else \"ðŸ’» Coding Mode: OFF\"\n",
    "        return new_mode, gr.update(value=label)\n",
    "\n",
    "    coding_btn.click(fn=toggle_coding, inputs=coding_mode, outputs=[coding_mode, coding_btn])\n",
    "\n",
    "    hidden_file = gr.File(visible=False, file_types=[\".json\"])\n",
    "\n",
    "    \n",
    "    hidden_file.change(fn=load_chat, inputs=hidden_file, outputs=[state, chatbot])\n",
    "    save_btn.click(fn=save_chat, inputs=state, outputs=[])\n",
    "\n",
    "    \n",
    "    txt.submit(fn=chat, inputs=[txt, state, coding_mode], outputs=[chatbot, txt])\n",
    "    send.click(fn=chat, inputs=[txt, state, coding_mode], outputs=[chatbot, txt])\n",
    "    state.change(fn=lambda h: [[u, a] for u, a, _ in h], inputs=state, outputs=chatbot)\n",
    "\n",
    "\n",
    "    \n",
    "    open_btn.click(None, None, None, js=\"() => document.querySelector('input[type=file]').click()\")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616cc056-4ffc-4c88-ae3e-9aa57ffa46e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310-cuda]",
   "language": "python",
   "name": "conda-env-py310-cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
